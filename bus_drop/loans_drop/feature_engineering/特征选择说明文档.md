# 过滤法
比较简单，它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征。上面
- 方差筛选
- 相关系数筛选
- 假设检验 卡方检验(sklearn.chi2)、F检验(sklearn.f_classif/f_regression)、t检验
- 互信息 sklearn.mutual_info_classif/mutual_info_regression计算输入特征和输出之间的互信息  

在没有思路的时候,优先使用卡方检验和互信息
# 包装法 
根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征。
- 递归消除特征法/反向特征消除rfe(recursive feature elimination) 
   - 使用机器学习模型来进行多轮训练，每轮训练消除若干权值系数的对应特征
   - 基于新的特征集进行下一轮训练
   
- SVM-RFE以支持向量机来做RFE的机器学习模型选择特征。
   - 第一轮训练的时候，会选择所有的特征来训练，得到了分类的超平面 wx + b = 0 后，如果有n个特征，那么RFE-SVM会选择出w中分量的平方值
最小的那个序号i对应的特征，将其排除
   - 在第二类的时候，特征数就剩下n-1个了，我们继续用这n-1个特征和输出值来训练SVM，同样的，去掉最小的那个序号i对应的特征。
   - 以此类推，直到剩下的特征数满足我们的需求为止。
# 嵌入法
特点： 
- 它先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小来选择特征。
- 和RFE的区别是它不是通过不停的筛掉特征来进行训练，而是使用的都是特征全集。
- 在sklearn中，使用SelectFromModel函数来选择特征。

过程：
- 最常用的是使用L1正则化和L2正则化来选择特征。正则化惩罚项越大，那么模型的系数就会越小。当正则化惩罚项大到一定的程度
的时候，部分特征系数会变成0，
- 当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0. 
- 但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。
- 常用的L1正则化和L2正则化来选择特征的基学习器是逻辑回归。
- 也可以使用决策树或者GBDT。
- 不是所有的机器学习方法都可以作为嵌入法的基学习器呢，可以得到特征系数coef或者可以得到特征重要度(feature
importances)的算法才可以做为嵌入法的基学习器。

# 寻找高级特征
车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个二级特征。根据车的速度特征，我们
就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加速度这
个四级特征。。。也就是说，高级特征可以一直寻找下去。
- 1.若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最
近的7天的销售额相加得到。
- 2.若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前
一月内的销售额。
- 3.若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特
征。
- 4.若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户
平均每件商品的销售额。
- 随便的两两组合形成高级特征，这样容易导致特征爆炸，反而没有办法得到较好的模型。
- 个人经验是，聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点。